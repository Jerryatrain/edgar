{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:04:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Identity of the Edgar REST client set to <span style=\"font-weight: bold\">[</span>8049qq.com@gmail.com<span style=\"font-weight: bold\">]</span>                     <a href=\"file://c:\\Users\\TJR\\anaconda3\\envs\\edgar\\Lib\\site-packages\\edgar\\core.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">core.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\TJR\\anaconda3\\envs\\edgar\\Lib\\site-packages\\edgar\\core.py#158\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">158</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02:04:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Identity of the Edgar REST client set to \u001b[1m[\u001b[0m8049qq.com@gmail.com\u001b[1m]\u001b[0m                     \u001b]8;id=250438;file://c:\\Users\\TJR\\anaconda3\\envs\\edgar\\Lib\\site-packages\\edgar\\core.py\u001b\\\u001b[2mcore.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=425769;file://c:\\Users\\TJR\\anaconda3\\envs\\edgar\\Lib\\site-packages\\edgar\\core.py#158\u001b\\\u001b[2m158\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from edgar import *\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import re\n",
    "import multiprocessing\n",
    "set_identity('8049qq.com@gmail.com')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a regex function, it requires at least a verd and a noun exist, and their distance is less than 20 words \n",
    "# verbs_1 = ['freeze', 'froze', 'frozen', 'freezing', 'close', 'closed', 'discontinue', 'discontinued', 'terminate', 'terminated', 'renegotiate', 'renegotiated']\n",
    "# nouns_1 = ['defined benefit', 'pension plan', 'retirement', 'postretirement', 'postemployment', 'pension', 'benefit']\n",
    "\n",
    "# # Create a regex pattern\n",
    "# pattern_1 = r'\\b(?:' + '|'.join(re.escape(word) for word in verbs_1) + r')\\b(?:\\W+\\w+){0,20}?\\W+\\b(?:' + '|'.join(re.escape(word) for word in nouns_1) + r')\\b' + \\\n",
    "#             r'|\\b(?:' + '|'.join(re.escape(word) for word in nouns_1) + r')\\b(?:\\W+\\w+){0,20}?\\W+\\b(?:' + '|'.join(re.escape(word) for word in verbs_1) + r')\\b'\n",
    "\n",
    "\n",
    "# Define lists of words\n",
    "verbs_1 = ['freeze', 'froze', 'frozen', 'freezing']\n",
    "nouns_1 = ['defined benefit', 'pension plan', 'retirement', 'postretirement', 'postemployment', 'pension', 'benefit']\n",
    "years = [str(year) for year in range(1985, 2026)]\n",
    "words_distance = 15\n",
    "\n",
    "# Create a regex pattern\n",
    "pattern_1 = r'\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(year) for year in years) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(verb) for verb in verbs_1) + r')\\b(?:\\W+\\w+){0,' + str(words_distance) + r'}\\W+\\b(?:' + '|'.join(re.escape(noun) for noun in nouns_1) + r')\\b'\n",
    "\n",
    "\n",
    "# for pattern 2\n",
    "verbs_2 = ['move', 'moved', 'transfer', 'transferred', 'transfering', 'transfered', 'transfering', 'transfered' ,'transit', 'transition','transited','change','changed','turn','turned','new employee']\n",
    "nouns_2 = ['defined benefit', 'pension plan', 'retirement', 'postretirement', 'defined contribution', 'contribution', '401(k)']\n",
    "\n",
    "# pattern 2\n",
    "pattern_2 = r'\\b(?:' + '|'.join(re.escape(word) for word in verbs_2) + r')\\b(?:\\W+\\w+){0,20}?\\W+\\b(?:' + '|'.join(re.escape(word) for word in nouns_2) + r')\\b' + \\\n",
    "            r'|\\b(?:' + '|'.join(re.escape(word) for word in nouns_2) + r')\\b(?:\\W+\\w+){0,20}?\\W+\\b(?:' + '|'.join(re.escape(word) for word in verbs_2) + r')\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for year in range(2000,2021):\n",
    "\n",
    "    # open pcikle file to get the freeze collection\n",
    "    with open(f'data/freeze_collection_{year}_alternative.pkl', 'rb') as f:\n",
    "        freeze_collection = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    freeze_all = {}\n",
    "    # loop through the freeze collection, key is the filing, value is (result 1, result 2)\n",
    "    for key, value in freeze_collection.items():\n",
    "\n",
    "        # for each filing, we store the filing information first\n",
    "        file_info = {'form': key.form,\n",
    "                    'date': key.filing_date,\n",
    "                    'cik': key.cik,\n",
    "                    'company': key.company,\n",
    "                    'link': key.document.url\n",
    "                    }\n",
    "        \n",
    "        # for each filing, there are results 1 and 2 which may contain multiple sections\n",
    "        result_1 = {}\n",
    "        if (value[0] is not None and value[0]['sections']): # if there is a result 1\n",
    "            for n, section in enumerate(value[0]['sections']): # loop through each section\n",
    "                # identify whether the section is a table, if there is too much \"|\" in the section, then it is a table\n",
    "                if section['doc'].count('|') > 20:\n",
    "                    continue\n",
    "\n",
    "                # find the keywords in the section\n",
    "                matches_1 = re.findall(pattern_1, section['doc'], re.IGNORECASE)\n",
    "                serp_search = re.search('SERP', section['doc'], re.IGNORECASE)\n",
    "\n",
    "                if serp_search:\n",
    "                    serp_index = 1\n",
    "                else:\n",
    "                    serp_index = 0\n",
    "                \n",
    "                # store the section information\n",
    "                section_dict = {\n",
    "                    'doc' : section['doc'],\n",
    "                    'keywords': matches_1,\n",
    "                    'serp': serp_index\n",
    "                }\n",
    "                \n",
    "                result_1[f'section_{n}'] = section_dict\n",
    "            \n",
    "        \n",
    "        # same for results 2\n",
    "        result_2 = {}\n",
    "        if (value[1] is not None and value[0]['sections']):\n",
    "            for n, section in enumerate(value[1]['sections']):\n",
    "                if section['doc'].count('|') > 20:\n",
    "                    continue\n",
    "\n",
    "                matches_2 = re.findall(pattern_2, section['doc'], re.IGNORECASE)\n",
    "                \n",
    "                \n",
    "                section_dict = {\n",
    "                    'doc' : section['doc'],\n",
    "                    'keywords': matches_2\n",
    "                }\n",
    "                \n",
    "                result_2[f'section_{n}'] = section_dict\n",
    "\n",
    "\n",
    "    #    if both results are empty, then we don't need to store this record\n",
    "        if not result_1 and not result_2:\n",
    "            continue\n",
    "\n",
    "        freeze_all[key.accession_no] = {\n",
    "            'file_info': file_info,\n",
    "            'result_1': result_1,\n",
    "            'result_2': result_2\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    with open(f'data/summary_keywords_{year}_alternative.pkl', 'wb') as f:\n",
    "        pickle.dump(freeze_all, f)\n",
    "\n",
    "    # transform the dictionary to a table for search 1\n",
    "    table_all_1 = {}\n",
    "    for key, value in freeze_all.items(): # key is filing accession no, value contains three part, file_info, result_1, result_2\n",
    "        if not value['result_1']: # if there is no result 1, then try next\n",
    "            continue\n",
    "\n",
    "        col_dict = value['file_info'] # we get the file information first\n",
    "\n",
    "        section_list = [] # store the section information in a list instead of a dictionary, so we can explode it later and turn it into a table\n",
    "        for section, content in value['result_1'].items():\n",
    "            section_list.append((content['doc'],content['keywords'], content['serp'])) # store the doc, keywords, and serp information\n",
    "\n",
    "        col_dict['section'] = section_list\n",
    "\n",
    "        table_all_1[key] = col_dict\n",
    "\n",
    "    # exploe the section and keyword list, and turn it into a table\n",
    "    table_1 = pd.DataFrame(table_all_1).T.explode('section')\n",
    "    table_1[['doc', 'keyword','serp']]= table_1['section'].apply(pd.Series)\n",
    "    table_1.drop(columns = ['section'], inplace = True)\n",
    "    table_1.to_excel(f'data/search1_result_{year}_alternative.xlsx')\n",
    "\n",
    "    # # transform the dictionary to a table for search 2 \n",
    "    # table_all_2 = {}\n",
    "    # for key, value in freeze_all.items():\n",
    "    #     if not value['result_2']:\n",
    "#         continue\n",
    "\n",
    "#     col_dict = value['file_info']\n",
    "\n",
    "#     section_list = []\n",
    "#     for section, content in value['result_2'].items():\n",
    "#         section_list.append((content['doc'],content['keywords']))\n",
    "\n",
    "#     col_dict['section'] = section_list\n",
    "#     table_all_2[key] = col_dict\n",
    "\n",
    "# # exploe the section and keyword list, and turn it into a table\n",
    "# table_2 = pd.DataFrame(table_all_2).T.explode('section')\n",
    "# table_2[['doc', 'keyword']]= table_2['section'].apply(pd.Series)\n",
    "# table_2.drop(columns = ['section'], inplace = True)\n",
    "# table_2.to_excel(f'data/search2_result_{year}_alternative.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search1_all = pd.DataFrame()\n",
    "for year in range(2000,2021):\n",
    "    tmp = pd.read_excel(f'data/search1_result_{year}_alternative.xlsx', index_col=0)\n",
    "    tmp['year'] = year\n",
    "    search1_all = pd.concat([search1_all, tmp])\n",
    "search1_all.sort_values(by='year', ascending=False).to_excel('data/search1_all_alternative.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>date</th>\n",
       "      <th>cik</th>\n",
       "      <th>company</th>\n",
       "      <th>link</th>\n",
       "      <th>doc</th>\n",
       "      <th>keyword</th>\n",
       "      <th>serp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [form, date, cik, company, link, doc, keyword, serp]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(f'data/search1_result_2019_alternative.xlsx', index_col=0)\n",
    "df[df.company.str.contains('AMERICAN Airlines')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cik==4515].to_excel('data/american_airlines_2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
